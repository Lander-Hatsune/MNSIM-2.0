{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer.py对照\n",
    "检查quantize和layers能否保证在各个模式下完全一致，包括structure_forward。\n",
    "首先，利用暂存的Lenet权重，提取第一层并保存，并记录相关信息，同时随机生成一组input作为简单的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "weights = torch.load(\"../zoo/cifar10_lenet_params.pth\")\n",
    "layer1 = dict()\n",
    "for name, params in weights.items():\n",
    "    if name.startswith(\"layer_list.0\"):\n",
    "        layer1[name[len(\"layer_list.0.\"):]] = params\n",
    "# save layer1\n",
    "torch.save(layer1, \"../zoo/cifar10_lenet_layer1.pth\")\n",
    "# save input\n",
    "input = torch.clamp(torch.randn(1, 3, 6, 6), 0, 1)\n",
    "torch.save(input, \"../zoo/cifar10_lenet_input.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import MNSIM.Interface.quantize as quantize\n",
    "from MNSIM.Interface.quantize import QuantizeLayer\n",
    "hardware_config = {\n",
    "    \"xbar_size\": 512,\n",
    "    \"input_bit\": 2,\n",
    "    \"weight_bit\": 1,\n",
    "    \"quantize_bit\": 10,\n",
    "}\n",
    "layer_config = {\n",
    "    \"type\": \"conv\",\n",
    "    \"in_channels\": 3,\n",
    "    \"out_channels\": 6,\n",
    "    \"kernel_size\": 5,\n",
    "}\n",
    "quantize_config = {\n",
    "    \"weight_bit\": 9,\n",
    "    \"activation_bit\": 9,\n",
    "    \"point_shift\": -2,\n",
    "}\n",
    "layer = QuantizeLayer(hardware_config, layer_config, quantize_config)\n",
    "layer.load_state_dict(torch.load(\"../zoo/cifar10_lenet_layer1.pth\"))\n",
    "input = torch.load(\"../zoo/cifar10_lenet_input.pth\")\n",
    "# forward\n",
    "quantize.last_activation_scale = 1 / 255.\n",
    "quantize.last_activation_bit = 9\n",
    "layer.eval()\n",
    "output = layer.forward(input, method=\"SINGLE_FIX_TEST\")\n",
    "print(output)\n",
    "torch.save(output, \"quantize.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from MNSIM.Interface.layer import BaseWeightLayer\n",
    "layer_ini = {\n",
    "    \"layer\": {\n",
    "        \"in_channels\": 3,\n",
    "        \"out_channels\": 6,\n",
    "        \"kernel_size\": 5,\n",
    "    },\n",
    "    \"quantize\": {\n",
    "        \"input\": 9,\n",
    "        \"weight\": 9,\n",
    "        \"output\": 9,\n",
    "    },\n",
    "    \"hardware\": {\n",
    "        \"xbar_row\": 512,\n",
    "        \"cell_bit\": 1,\n",
    "        \"dac_bit\": 2,\n",
    "        \"adc_bit\": 10,\n",
    "        \"point_shift\": -2,\n",
    "    }\n",
    "}\n",
    "layer = BaseWeightLayer.get_class_(\"conv\")(layer_ini)\n",
    "layer.load_state_dict(torch.load(\"../zoo/cifar10_lenet_layer1.pth\"), strict=False)\n",
    "input = torch.load(\"../zoo/cifar10_lenet_input.pth\")\n",
    "# forward\n",
    "layer.eval()\n",
    "input_config = [torch.FloatTensor([9, 1/255.])]\n",
    "output = layer.forward(input, method=\"SINGLE_FIX_TEST\", input_config=input_config)\n",
    "print(output)\n",
    "torch.save(output, \"layer.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.load(\"layer.pth\")\n",
    "b = torch.load(\"quantize.pth\")\n",
    "print(torch.max(a-b))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "336c25dfa027737acb76905ce3eb6d17346417a81c7e61a9cda13b7f69491512"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('mnsim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
